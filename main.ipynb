{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, LeaveOneOut\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "train_file_path = '/home/data/1.生产参数记录表/生产参数记录表-2018年4月/'\n",
    "test_file_path = '/home/data/1.生产参数记录表/生产参数记录表-2018年5月/'\n",
    "temp_file_path = '/home/jovyan/work/'\n",
    "\n",
    "X_list =   ['D101硫酸泵电流-Raw.csv',          'T101到T102洗涤液流量-Raw.csv',\n",
    "            'E301干燥窑电流-Raw.csv',          'T101进口稀磷酸流量-Raw.csv',\n",
    "            'E301干燥窑转速-Raw.csv',          'T102Һλ-Raw.csv',\n",
    "            'E301干燥窑进口烟气温度-Raw.csv',  'T102进口浓磷酸流量-Raw.csv',\n",
    "            'F101热风炉炉膛温度-Raw.csv',      'Z220造粒机电流-Raw.csv',\n",
    "            'F101热风炉鼓风压力-Raw.csv',      'Z221进口液氨压力-Raw.csv',\n",
    "            'F101鼓风机电流-Raw.csv',          'Z221进口液氨流量-Raw.csv',\n",
    "            'G132洗涤塔液位-Raw.csv',          'Z230进口洗涤液压力-Raw.csv',\n",
    "            'L101斗提机电流-Raw.csv',          'Z230进口洗涤液浓度-Raw.csv',\n",
    "            'L102返料皮带电流-Raw.csv',        'Z230进口液氨压力-Raw.csv',\n",
    "            'L103斗提机电流-Raw.csv',          'Z230进口液氨流量-Raw.csv',\n",
    "            'S101A振网筛电流-Raw.csv',         'Z230进口硫酸压力-Raw.csv',\n",
    "            'S101B振网筛电流-Raw.csv',         '内染剂泵流量-Raw.csv',\n",
    "            'S102A破碎机电流1-Raw.csv',        '成品重量-Raw.csv',\n",
    "            'S102A破碎机电流2-Raw.csv',        '浓硫酸流量-Raw.csv',\n",
    "            'S102B破碎机电流1-Raw.csv',        '液氨压力-Raw.csv',\n",
    "            'S102B破碎机电流2-Raw.csv',        '液氨流量-Raw.csv',\n",
    "            'T101Һλ-Raw.csv',                 '返料重量-Raw.csv']\n",
    "\n",
    "train_label_file = '/home/data/2.产品检验报告/产品检验报告2018-4-1.csv'\n",
    "test_label_file = '/home/data/2.产品检验报告/产品检验报告2018-5-1-sample.csv'\n",
    "\n",
    "train_df = {}\n",
    "test_df = {}\n",
    "for f in X_list:\n",
    "    train_df[f] = pd.read_csv(train_file_path+f, header='infer', encoding='gbk')\n",
    "    test_df[f] = pd.read_csv(test_file_path+f, header='infer', encoding='gbk')\n",
    "\n",
    "y_train = pd.read_csv(train_label_file, header='infer', encoding='gbk')\n",
    "y_test = pd.read_csv(test_label_file, header='infer', encoding='gbk')\n",
    "\n",
    "model_path = '/home/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing 1: Add corresponding y_id to X dataframe\n",
    "\n",
    "if not os.path.exists(temp_file_path+'train_with_id/'): \n",
    "    os.makedirs(temp_file_path+'train_with_id/')\n",
    "if not os.path.exists(temp_file_path+'test_with_id/'): \n",
    "    os.makedirs(temp_file_path+'test_with_id/')\n",
    "    \n",
    "y_train = pd.read_csv(train_label_file, header='infer', encoding='gbk')\n",
    "y_test = pd.read_csv(test_label_file, header='infer', encoding='gbk')\n",
    "    \n",
    "# ———————————————— 训练集id对齐 ————————————————\n",
    "time_period = []\n",
    "for t in y_train.iloc[:,1]:    # 训练集label所属时间段\n",
    "    d = re.search(r'\\d+.\\d+', t).group(0)   # 匹配日期\n",
    "    t1 = re.search(r'(\\d{2}:\\d{2}) - (\\d{2}:\\d{2})', t).group(1)   # 匹配起始时间点\n",
    "    t2 = re.search(r'(\\d{2}:\\d{2}) - (\\d{2}:\\d{2})', t).group(2)   # 匹配结束时间点\n",
    "    strtime = '2018.%s %s:00'%(d,t1)  \n",
    "    y_ts1 = int(time.mktime(time.strptime(strtime, \"%Y.%m.%d %H:%M:%S\")))  # 转化为时间戳\n",
    "    strtime = '2018.%s %s:00'%(d,t2)\n",
    "    y_ts2 = int(time.mktime(time.strptime(strtime, \"%Y.%m.%d %H:%M:%S\")))\n",
    "    if t2 == '00:00':          # 结束时间点为0点的，日期加1\n",
    "        y_ts2 += 3600*24\n",
    "    time_period.append((y_ts1, y_ts2))\n",
    "\n",
    "for f in X_list:\n",
    "    if os.path.exists(temp_file_path+'train_with_id/'+f):   # 文件已存在则跳过\n",
    "        continue\n",
    "    print('handling %s file of trainset...'%f)\n",
    "    train_df[f]['id'] = None\n",
    "    for i in range(len(train_df[f])):  \n",
    "        if i % 1000 == 0:\n",
    "            print('handling %d/%d record'%(i, len(train_df[f])))\n",
    "        if type(train_df[f].iloc[i,1]) == str:    # 排除缺失值\n",
    "            strtime = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}',train_df[f].iloc[i,1]).group(0)  # 提取样本数据的测定时间点\n",
    "            x_ts = int(time.mktime(time.strptime(strtime, \"%Y-%m-%d %H:%M:%S\")))                   # 转化为时间戳\n",
    "            for j in range(len(time_period)):     # 遍历label数据所有的时间段，找出x对应的时间\n",
    "                if time_period[j][0]<=x_ts and x_ts<=time_period[j][1]:\n",
    "                    train_df[f].loc[i,'id'] = y_train.iloc[j,0]\n",
    "                    break\n",
    "    train_df[f].to_csv(temp_file_path+'train_with_id/'+f, index = False, encoding='utf-8')\n",
    "\n",
    "# ———————————————— 测试集id对齐 ————————————————\n",
    "time_period = []\n",
    "for t in y_test.iloc[:,1]:    # 训练集label所属时间段\n",
    "    d = re.search(r'\\d+.\\d+', t).group(0)   # 匹配日期\n",
    "    t1 = re.search(r'(\\d{2}:\\d{2}) - (\\d{2}:\\d{2})', t).group(1)   # 匹配起始时间点\n",
    "    t2 = re.search(r'(\\d{2}:\\d{2}) - (\\d{2}:\\d{2})', t).group(2)   # 匹配结束时间点\n",
    "    strtime = '2018.%s %s:00'%(d,t1)  \n",
    "    y_ts1 = int(time.mktime(time.strptime(strtime, \"%Y.%m.%d %H:%M:%S\")))  # 转化为时间戳\n",
    "    strtime = '2018.%s %s:00'%(d,t2)\n",
    "    y_ts2 = int(time.mktime(time.strptime(strtime, \"%Y.%m.%d %H:%M:%S\")))\n",
    "    if t2 == '00:00':          # 结束时间点为0点的，日期加1\n",
    "        y_ts2 += 3600*24\n",
    "    time_period.append((y_ts1, y_ts2))\n",
    "\n",
    "for f in X_list:\n",
    "    if os.path.exists(temp_file_path+'test_with_id/'+f):   # 文件已存在则跳过\n",
    "        continue\n",
    "    print('handling %s file of testset...'%f)\n",
    "    test_df[f]['id'] = None\n",
    "    for i in range(len(test_df[f])):  \n",
    "        if i % 1000 == 0:\n",
    "            print('handling %d/%d record'%(i, len(test_df[f])))\n",
    "        if type(test_df[f].iloc[i,1]) == str:    # 排除缺失值\n",
    "            strtime = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}',test_df[f].iloc[i,1]).group(0)  # 提取样本数据的测定时间点\n",
    "            x_ts = int(time.mktime(time.strptime(strtime, \"%Y-%m-%d %H:%M:%S\")))                   # 转化为时间戳\n",
    "            for j in range(len(time_period)):     # 遍历label数据所有的时间段，找出x对应的时间\n",
    "                if time_period[j][0]<=x_ts and x_ts<=time_period[j][1]:\n",
    "                    test_df[f].loc[i,'id'] = y_test.iloc[j,0]\n",
    "                    break\n",
    "    test_df[f].to_csv(temp_file_path+'test_with_id/'+f, index = False, encoding='utf-8')\n",
    "\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing 2: 取36个检测属性的均值、方差等统计特征作为训练特征\n",
    "\n",
    "if not os.path.exists(temp_file_path+'preprocessed_data/'): \n",
    "    os.makedirs(temp_file_path+'preprocessed_data/')\n",
    "    \n",
    "y_train = pd.read_csv(train_label_file, header='infer', encoding='gbk')\n",
    "y_test = pd.read_csv(test_label_file, header='infer', encoding='gbk')\n",
    "    \n",
    "train_df_with_id = {}\n",
    "test_df_with_id = {}\n",
    "for f in X_list:\n",
    "    train_df_with_id[f] = pd.read_csv(temp_file_path+'train_with_id/'+f,  header='infer', encoding='utf-8')\n",
    "    test_df_with_id[f] = pd.read_csv(temp_file_path+'test_with_id/'+f,  header='infer', encoding='utf-8')\n",
    "\n",
    "train_data_df = pd.DataFrame.copy(y_train)\n",
    "test_data_df = pd.DataFrame.copy(y_test)\n",
    "    \n",
    "for f in X_list:\n",
    "    for i in range(len(train_data_df)):\n",
    "        df = train_df_with_id[f]\n",
    "        target_data = df[df['id']==train_data_df.loc[i,'id']].iloc[:,2]\n",
    "        mean = target_data.mean()\n",
    "        std = target_data.std()\n",
    "        target_data[target_data<mean-3*std] = mean        # 以均值替代异常值\n",
    "        target_data[target_data>mean+3*std] = mean\n",
    "        \n",
    "        train_data_df.loc[i,f+'_mean'] = target_data.mean()   # 取均值\n",
    "#         train_data_df.loc[i,f+'_std'] = target_data.std()     # 取方差\n",
    "#         train_data_df.loc[i,f+'_max'] = target_data.max() # 取最大值\n",
    "#         train_data_df.loc[i,f+'_min'] = target_data.min() # 取最小值\n",
    "#         train_data_df.loc[i,f+'_median'] = target_data.median() # 中位数\n",
    "#         train_data_df.loc[i,f+'_max-min'] = train_data_df.loc[i,f+'_max']-train_data_df.loc[i,f+'_min'] \n",
    "                                    \n",
    "        if i < len(test_data_df):\n",
    "            df = test_df_with_id[f]\n",
    "            target_data = df[df['id']==test_data_df.loc[i,'id']].iloc[:,2]\n",
    "            mean = target_data.mean()\n",
    "            std = target_data.std()\n",
    "            target_data[target_data<mean-3*std] = mean        # 以均值替代异常值\n",
    "            target_data[target_data>mean+3*std] = mean\n",
    "            \n",
    "            test_data_df.loc[i,f+'_mean'] = target_data.mean()   # 取均值\n",
    "#             test_data_df.loc[i,f+'_std'] = target_data.std()     # 取方差\n",
    "#             test_data_df.loc[i,f+'_max'] = target_data.max()    # 取最大值\n",
    "#             test_data_df.loc[i,f+'_min'] = target_data.min()    # 取最小值\n",
    "#             test_data_df.loc[i,f+'_median'] = target_data.median() # 中位数\n",
    "#             test_data_df.loc[i,f+'_max-min'] = test_data_df.loc[i,f+'_max']-test_data_df.loc[i,f+'_min']\n",
    "    print('finish '+ f)\n",
    "    \n",
    "train_data_df.to_csv(temp_file_path+'preprocessed_data/train_mean.csv', index = False, encoding='utf-8')\n",
    "test_data_df.to_csv(temp_file_path+'preprocessed_data/test_mean.csv', index = False, encoding='utf-8')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 36) (5, 141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train 1: 准备训练数据，请先运行预处理模块产生所需数据\n",
    "\n",
    "train_data_df = pd.read_csv(temp_file_path+'preprocessed_data/train_mean.csv', header='infer', encoding='utf-8')\n",
    "test_data_df = pd.read_csv(temp_file_path+'preprocessed_data/test_mean.csv', header='infer', encoding='utf-8')\n",
    "\n",
    "# K = None         # 保留特征个数，None表示全部保留\n",
    "# r_stds = []     # 保存各特征的归一化标准差，筛选出归一化标准差最大的前K各特征\n",
    "# for i in range(7, len(train_data_df.columns)):\n",
    "#     mean = train_data_df.iloc[:,i].mean()\n",
    "#     r_std = (train_data_df.iloc[:,i]/mean).std()    # 计算归一化之后的标准差\n",
    "#     r_stds.append((r_std,i))\n",
    "# r_stds.sort(reverse = True)\n",
    "# f_index = list(list(zip(*r_stds[:K]))[1])  # 获得前K重要的特征列index\n",
    "\n",
    "new_train_data_df = pd.merge(train_data_df.iloc[:,:7], train_data_df.iloc[:, f_index], left_index=True, right_index=True)\n",
    "new_test_data_df = pd.merge(test_data_df.iloc[:,:7], test_data_df.iloc[:, f_index], left_index=True, right_index=True)\n",
    "\n",
    "# new_train_data_df['返料比'] = new_train_data_df['返料重量-Raw.csv_mean']/new_train_data_df['成品重量-Raw.csv_mean']\n",
    "# new_test_data_df['返料比'] = new_test_data_df['返料重量-Raw.csv_mean']/new_test_data_df['成品重量-Raw.csv_mean']\n",
    "# new_train_data_df['返料差'] = new_train_data_df['返料重量-Raw.csv_mean']-new_train_data_df['成品重量-Raw.csv_mean']\n",
    "# new_test_data_df['返料差'] = new_test_data_df['返料重量-Raw.csv_mean']-new_test_data_df['成品重量-Raw.csv_mean']\n",
    "\n",
    "X_train = new_train_data_df.iloc[:,7:].values\n",
    "X_test = new_test_data_df.iloc[:,7:].values\n",
    "\n",
    "valid_sample_id = [i for i in range(len(X_train))]    # 移除特征值全空的样本\n",
    "for i in range(len(X_train)):\n",
    "    if np.isnan(X_train[i]).all():\n",
    "        valid_sample_id.remove(i)\n",
    "X_train = X_train[valid_sample_id]\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)  # 以均值填补缺失值\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.fit_transform(X_test)\n",
    "\n",
    "y_train_df = pd.read_csv(train_label_file, header='infer', encoding='gbk')\n",
    "y_test_df = pd.read_csv(test_label_file, header='infer', encoding='gbk')\n",
    "y_train = y_train_df.iloc[:,2:7].values[valid_sample_id].T\n",
    "# y_train = y_train_df.iloc[:,2:7].values.T\n",
    "y_test = y_test_df.iloc[:,2:7].values.T\n",
    "\n",
    "# 通过对训练集聚类来对分层样本\n",
    "random_seed = 0\n",
    "cluster_model = KMeans(n_clusters=6, random_state=random_seed).fit(X_train)  # KMeans聚类\n",
    "cluster_labels = cluster_model.labels_ \n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化聚类效果\n",
    "def plot_embedding(X, title=None): \n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0) \n",
    "    X = (X - x_min) / (x_max - x_min)    #正则化 \n",
    "    # print(X)\n",
    "    plt.figure(figsize=(10, 10)) \n",
    "    ax = plt.subplot(111) \n",
    "    for i in range(X.shape[0]): \n",
    "        plt.text(X[i, 0], X[i, 1], '*', color=plt.cm.Set1(labels[i]), fontdict={'weight': 'bold', 'size': 12}) \n",
    "    plt.xticks([]), plt.yticks([]) \n",
    "    if title is not None: \n",
    "        plt.title(title)\n",
    "\n",
    "cluster_model = KMeans(n_clusters=6, random_state=0).fit(X_train)\n",
    "cluster_labels = cluster_model.labels_\n",
    "print(calinski_harabaz_score(X_train, cluster_labels))\n",
    "\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "X_train_tsne = tsne.fit_transform(X_train)\n",
    "plot_embedding(X_train_tsne)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on validation set of phosphorus_content: 0.26570\n",
      "The RMSE on validation set of nitrogen_content: 0.13536\n",
      "The RMSE on validation set of total_nutrient: 0.25411\n",
      "The RMSE on validation set of water_content: 0.22303\n",
      "The RMSE on validation set of particle_size: 2.49567\n"
     ]
    }
   ],
   "source": [
    "# Train 2-2*: lightGBM 留一法验证模型\n",
    "label_list = y_test_df.columns.values[2:].tolist()\n",
    "feature_list = train_data_df.columns[7:]\n",
    "\n",
    "epochs = [30, 125, 30, 70, 25]\n",
    "for i in range(len(label_list)):\n",
    "    k = 0\n",
    "    rmse = []\n",
    "    loo = LeaveOneOut()    # 留一法划分数据集\n",
    "    for train_index, valid_index in loo.split(X_train):\n",
    "        k += 1\n",
    "        X_train_t, X_valid_t = X_train[train_index], X_train[valid_index]\n",
    "        y_train_t, y_valid_t = y_train.T[train_index], y_train.T[valid_index]\n",
    "        \n",
    "        model = lgb.LGBMRegressor(objective='regression',max_depth=-1, num_leaves=64,learning_rate=0.05,\\\n",
    "                                  n_estimators=epochs[i], bagging_freq=5, bagging_feaction=1.0, feature_fraction=0.7)\n",
    "        model.fit(X_train_t, y_train_t.T[i], eval_metric='rmse')\n",
    "        y_pred_valid = model.predict(X_valid_t)\n",
    "        rmse.append(mean_squared_error(y_valid_t.T[i], y_pred_valid) ** 0.5)               # 计算在验证集上的RMSE\n",
    "#         if k%10 == 0:    \n",
    "#             print('training %s model %d times...'%(label_list[i], k))\n",
    "\n",
    "    score = np.mean(rmse)\n",
    "    print('The RMSE on validation set of %s: %.5f'%(label_list[i],score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 36) (5, 141)\n",
      "Start training phosphorus_content model...\n",
      "Start saving phosphorus_content model...\n",
      "Start predicting phosphorus_content model...\n",
      "Start training nitrogen_content model...\n",
      "Start saving nitrogen_content model...\n",
      "Start predicting nitrogen_content model...\n",
      "Start training total_nutrient model...\n",
      "Start saving total_nutrient model...\n",
      "Start predicting total_nutrient model...\n",
      "Start training water_content model...\n",
      "Start saving water_content model...\n",
      "Start predicting water_content model...\n",
      "Start training particle_size model...\n",
      "Start saving particle_size model...\n",
      "Start predicting particle_size model...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Train 2-2*: lightGBM 全数据集训练模型，直接产生测试结果\n",
    "\n",
    "Re_train_model = True     # 是否从头训练模型，False的情况下，会尝试直接加载已存在的模型\n",
    "\n",
    "if not os.path.exists(temp_file_path+'model/'):\n",
    "    os.makedirs(temp_file_path+'model/')\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "label_list = y_test_df.columns.values[2:].tolist()\n",
    "\n",
    "result = {}\n",
    "epochs = [30, 125, 30, 70, 25]\n",
    "for i in range(len(label_list)):\n",
    "    if not Re_train_model and Path('%slightGBM_%s.model'%(temp_file_path+'model/', label_list[i])).exists():\n",
    "        print('Start loading %s model...'%label_list[i])\n",
    "        model = joblib.load('%slightGBM_%s.model'%(temp_file_path+'model/', label_list[i]))\n",
    "    else:\n",
    "        print('Start training %s model...'%label_list[i])\n",
    "        model = lgb.LGBMRegressor(objective='regression',max_depth=-1, num_leaves=64,learning_rate=0.05,\\\n",
    "                                  n_estimators=epochs[i], bagging_freq=5, bagging_feaction=1.0, feature_fraction=0.7)\n",
    "        model.fit(X_train, y_train[i])     # 全数据集训练模型\n",
    "        print('Start saving %s model...'%label_list[i])\n",
    "        joblib.dump(model, '%slightGBM_%s.model'%(temp_file_path+'model/', label_list[i]))\n",
    "        \n",
    "    print('Start predicting %s model...'%label_list[i])\n",
    "    result[label_list[i]] = model.predict(X_test)\n",
    "    \n",
    "    # ———————————————————————— 输出特征重要性 —————————————————————\n",
    "#     print('【Feature importances】')             \n",
    "#     importance = sorted(list(zip(list(model.feature_importances_), feature_list)), reverse=True) # 降序排序\n",
    "#     for f_id in range(len(feature_list)):\n",
    "#         if importance[f_id][0] == 0:\n",
    "#             break\n",
    "#         print(importance[f_id][1],\": \",importance[f_id][0])\n",
    "#     print('others: 0\\n')\n",
    "    # —————————————————————————————————————————————————————\n",
    "    \n",
    "result_df = pd.DataFrame.copy(y_test_df)\n",
    "for col in result.keys():\n",
    "    result_df[col] = result[col]\n",
    "\n",
    "result_df = result_df.drop(['product_batch'],axis=1)\n",
    "result_df.to_csv(temp_file_path+'LGBM_result.csv', index=False, encoding='utf-8')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on validation set of phosphorus_content: 0.26813\n",
      "The RMSE on validation set of nitrogen_content: 0.14766\n",
      "The RMSE on validation set of total_nutrient: 0.27125\n",
      "The RMSE on validation set of water_content: 0.22623\n",
      "The RMSE on validation set of particle_size: 2.38478\n"
     ]
    }
   ],
   "source": [
    "# Train 2-4*: SVR 留一法验证模型\n",
    "label_list = y_test_df.columns.values[2:].tolist()\n",
    "feature_list = train_data_df.columns[7:]\n",
    "\n",
    "C_list = [1, 1, 1, 1, 1]\n",
    "for i in range(len(label_list)):\n",
    "    k = 0\n",
    "    rmse = []\n",
    "    loo = LeaveOneOut()    # 留一法划分数据集\n",
    "    for train_index, valid_index in loo.split(X_train):\n",
    "        k += 1\n",
    "        X_train_t, X_valid_t = X_train[train_index], X_train[valid_index]\n",
    "        y_train_t, y_valid_t = y_train.T[train_index], y_train.T[valid_index]\n",
    "        \n",
    "        model = SVR(gamma='scale', C=C_list[i])\n",
    "        model.fit(X_train_t, y_train_t.T[i])\n",
    "        y_pred_valid = model.predict(X_valid_t)\n",
    "        rmse.append(mean_squared_error(y_valid_t.T[i], y_pred_valid) ** 0.5)               # 计算在验证集上的RMSE\n",
    "#         if k%10 == 0:    \n",
    "#             print('training %s model %d times...'%(label_list[i], k))\n",
    "\n",
    "    score = np.mean(rmse)\n",
    "    print('The RMSE on validation set of %s: %.5f'%(label_list[i],score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 36) (5, 141)\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Train 2-4*: SVR 全数据集训练模型，直接产生测试结果\n",
    "\n",
    "Re_train_model = True     # 是否从头训练模型，False的情况下，会尝试直接加载已存在的模型\n",
    "\n",
    "if not os.path.exists(temp_file_path+'model/'):\n",
    "    os.makedirs(temp_file_path+'model/')\n",
    "    \n",
    "label_list = y_test_df.columns.values[2:].tolist()\n",
    "\n",
    "result = {}\n",
    "C_list = [1, 1, 1, 1, 1]\n",
    "print(X_train.shape, y_train.shape)\n",
    "for i in range(len(label_list)):                    # 为每个指标单独训练一个模型\n",
    "    if not Re_train_model and Path('%sSVR_%s.model'%(temp_file_path+'model/', label_list[i])).exists():\n",
    "        print('Start loading %s model...'%label_list[i])\n",
    "        model = joblib.load('%sSVR_%s.model'%(temp_file_path+'model/', label_list[i]))\n",
    "    else:\n",
    "        model = SVR(gamma='scale', C=C_list[i])\n",
    "        model.fit(X_train, y_train[i])\n",
    "        joblib.dump(model, '%sSVR_%s.model'%(temp_file_path+'model/', label_list[i]))   # 保存模型\n",
    "        result[label_list[i]] = model.predict(X_test)\n",
    "\n",
    "result_df = pd.DataFrame.copy(y_test_df)    \n",
    "for col in result.keys():\n",
    "    result_df[col] = result[col]\n",
    "\n",
    "result_df = result_df.drop(['product_batch'],axis=1)\n",
    "result_df.to_csv(temp_file_path+'SVR_result.csv', index=False, encoding='utf-8')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on validation set of phosphorus_content: 0.27286\n",
      "The RMSE on validation set of nitrogen_content: 0.16811\n",
      "The RMSE on validation set of total_nutrient: 0.26776\n",
      "The RMSE on validation set of water_content: 0.22004\n",
      "The RMSE on validation set of particle_size: 2.48000\n"
     ]
    }
   ],
   "source": [
    "# Train 2-5*: KNN距离加权 留一法验证模型\n",
    "label_list = y_test_df.columns.values[2:].tolist()\n",
    "feature_list = train_data_df.columns[7:]\n",
    "\n",
    "def KNN_weighted_regression(X, y, test_sample, k = 3, metric = 'euclidean'):\n",
    "    KNN_index = [i for i in range(len(X))]\n",
    "    KNN_distance = paired_distances(X, np.asarray([test_sample]*len(X)).reshape((len(X),len(test_sample))), metric=metric)\n",
    "    order = sorted(zip(KNN_distance, KNN_index))\n",
    "    topK_distance, topK_index = zip(*order)\n",
    "    if topK_distance[0] == 0:\n",
    "        topK_distance = topK_distance[1:]\n",
    "        topK_index = topK_index[1:]\n",
    "    topK_distance = topK_distance[:k]\n",
    "    topK_index = topK_index[:k]\n",
    "    y_pred = sum([y[topK_index[i]]*(1/topK_distance[i]) for i in range(len(topK_index))])\n",
    "    y_pred /= sum(1/np.asarray(topK_distance))\n",
    "    return y_pred\n",
    "\n",
    "for i in range(len(label_list)):\n",
    "    k = 0\n",
    "    rmse = []\n",
    "    loo = LeaveOneOut()    # 留一法划分数据集\n",
    "    for train_index, valid_index in loo.split(X_train):\n",
    "        k += 1\n",
    "        X_train_t, X_valid_t = X_train[train_index], X_train[valid_index]\n",
    "        y_train_t, y_valid_t = y_train.T[train_index], y_train.T[valid_index]\n",
    "        \n",
    "        y_pred_valid = KNN_weighted_regression(X_train_t, y_train_t.T[i] ,X_valid_t[0], k=len(X_train_t), metric='cosine')\n",
    "        rmse.append(mean_squared_error(y_valid_t.T[i], [y_pred_valid]) ** 0.5)               # 计算在验证集上的RMSE\n",
    "#         if k%10 == 0:    \n",
    "#             print('training %s model %d times...'%(label_list[i], k))\n",
    "\n",
    "    score = np.mean(rmse)\n",
    "    print('The RMSE on validation set of %s: %.5f'%(label_list[i],score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 36) (5, 141)\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Train 2-5*: KNN距离加权 全数据集训练模型，直接产生测试结果\n",
    "\n",
    "label_list = y_test_df.columns.values[2:].tolist()\n",
    "\n",
    "result = {}\n",
    "print(X_train.shape, y_train.shape)\n",
    "for i in range(len(label_list)):                    # 为每个指标单独训练一个模型\n",
    "    y_pred = [KNN_weighted_regression(X_train, y_train[i] , x, k=len(X_train), metric='cosine') for x in X_test]\n",
    "    result[label_list[i]] = y_pred\n",
    "\n",
    "result_df = pd.DataFrame.copy(y_test_df)    \n",
    "for col in result.keys():\n",
    "    result_df[col] = result[col]\n",
    "\n",
    "result_df = result_df.drop(['product_batch'],axis=1)\n",
    "result_df.to_csv(temp_file_path+'KNN_result.csv', index=False, encoding='utf-8')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict lightGBM model...\n",
      "Start predict SVR model...\n",
      "Start predict KNN model...\n",
      "Start train stacking model...\n",
      "The RMSE on validation set of phosphorus_content: 0.16070\n",
      "The RMSE on validation set of nitrogen_content: 0.05993\n",
      "The RMSE on validation set of total_nutrient: 0.15313\n",
      "The RMSE on validation set of water_content: 0.10503\n",
      "The RMSE on validation set of particle_size: 1.71899\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# simple stacking 留一法找最优迭代\n",
    "\n",
    "stacking_model = ['lightGBM','SVR', 'KNN']\n",
    "\n",
    "label_list = y_test_df.columns.values[2:].tolist()\n",
    "\n",
    "train_set_predict = []\n",
    "test_set_predict = []\n",
    "\n",
    "for m in stacking_model:\n",
    "    if m == 'KNN':\n",
    "        continue\n",
    "    print('Start predict %s model...'%m)\n",
    "    for i in range(len(label_list)):\n",
    "        model = joblib.load('%s%s_%s.model'%(temp_file_path+'model/', m, label_list[i]))\n",
    "        train_set_predict.append(model.predict(X_train))\n",
    "        test_set_predict.append(model.predict(X_test))\n",
    "if 'KNN' in stacking_model:\n",
    "    print('Start predict %s model...'%'KNN')\n",
    "    for i in range(len(label_list)):\n",
    "        y_pred1 = [KNN_weighted_regression(X_train, y_train[i] , x, k=len(X_train), metric='cosine') for x in X_train]\n",
    "        y_pred2 = [KNN_weighted_regression(X_train, y_train[i] , x, k=len(X_train), metric='cosine') for x in X_test]\n",
    "        train_set_predict.append(y_pred1)\n",
    "        test_set_predict.append(y_pred2)    \n",
    "        \n",
    "train_set_predict = np.asarray(train_set_predict)\n",
    "test_set_predict = np.asarray(test_set_predict)\n",
    "\n",
    "result = {}\n",
    "rmse = []\n",
    "model_type = 'LGBM'\n",
    "\n",
    "epochs = [70, 100, 70, 70, 50]\n",
    "print('Start train %s model...'%'stacking')\n",
    "for i in range(len(label_list)):\n",
    "    k = 0\n",
    "    rmse = []\n",
    "    loo = LeaveOneOut()    # 留一法划分数据集\n",
    "    for train_index, valid_index in loo.split(X_train):\n",
    "        k += 1\n",
    "        X_train_s, X_valid_s = train_set_predict.T[train_index], train_set_predict.T[valid_index]\n",
    "        y_train_s, y_valid_s = y_train.T[train_index], y_train.T[valid_index]\n",
    "        if model_type == 'MLP': \n",
    "        # —————————————————--------—— MLP ——————————————--—————————————\n",
    "            num_iter = [500, 500, 500, 500, 100]\n",
    "            model = MLPRegressor(hidden_layer_sizes=(20), max_iter=num_iter[i], activation='relu', solver='lbfgs',\n",
    "                                 random_state=0)\n",
    "            model.fit(X_train_s, y_train_s.T[i])\n",
    "        # —————————————————————————————————————————————————————-\n",
    "        elif model_type == 'SVR':\n",
    "        # —————————————————--------—— SVR ——————————————--—————————————\n",
    "            model = SVR(gamma='scale')\n",
    "            model.fit(X_train_s, y_train_s.T[i])\n",
    "        # ————————————————————————————————————-—————————————————\n",
    "        elif model_type == 'LR':\n",
    "        # —————————————————--------——- LR ——————————————--—————————————\n",
    "            model = LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)\n",
    "            model.fit(X_train_s, y_train_s.T[i])\n",
    "        # —————————————————--------————-——————————————--—————————————\n",
    "        elif model_type == 'LGBM':\n",
    "        # —————————————————--------—— LGBM ————————————————————————————\n",
    "            model = lgb.LGBMRegressor(objective='regression',max_depth=-1, num_leaves=64,learning_rate=0.05, n_estimators=epochs[i])\n",
    "            model.fit(X_train_s, y_train_s.T[i])\n",
    "\n",
    "        # —————————————————--------————-——————————————--—————————————\n",
    "\n",
    "        # —————————————————--------—— ANN ————————————————————————————\n",
    "    #     model.add(Dense(10, input_shape=(X_train_s.shape[1],)))\n",
    "    #     model.add(BatchNormalization())\n",
    "    #     model.add(Activation('sigmoid'))\n",
    "    #     model.add(Dense(5, input_shape=(X_train_s.shape[1],)))\n",
    "    #     model.add(BatchNormalization())\n",
    "    #     model.add(Activation('sigmoid'))\n",
    "    #     model.add(Dense(1, activation='linear'))\n",
    "    #     model.compile(optimizer='sgd', loss='mse')\n",
    "    #     early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    #     reduce_LR = ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
    "    #     checkPoint = ModelCheckpoint(filepath='%sstacking_%s.model'%(temp_file_path+'model/', label_list[i]),\\\n",
    "    #                                  monitor='val_loss', save_best_only=True)\n",
    "    #     call_back_func = [early_stopping,reduce_LR, checkPoint]\n",
    "    #     model.fit(X_train_s, y_train_s, validation_data=(X_valid_s, y_valid_s), epochs=30, callbacks=call_back_func)\n",
    "\n",
    "    #     model = load_model(filepath='%sstacking_%s.model'%(temp_file_path+'model/', label_list[i]))\n",
    "        # ————————————————————————————————————-——————————————————\n",
    "   \n",
    "        y_pred_valid = model.predict(X_valid_s)\n",
    "        rmse.append(mean_squared_error(y_valid_s.T[i], y_pred_valid) ** 0.5)     # 计算在验证集上的RMSE\n",
    "#         if k%10 == 0:    \n",
    "#             print('training %s model %d times...'%(label_list[i], k))\n",
    "     \n",
    "    score = np.mean(rmse)\n",
    "    print('The RMSE on validation set of %s: %.5f'%(label_list[i],score)) \n",
    "print('done!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict lightGBM model...\n",
      "Start predict SVR model...\n",
      "Start predict KNN model...\n",
      "(141, 15)\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# simple stacking 输出测试结果\n",
    "\n",
    "stacking_model = ['lightGBM','SVR', 'KNN']\n",
    "\n",
    "label_list = y_test_df.columns.values[2:].tolist()\n",
    "\n",
    "train_set_predict = []\n",
    "test_set_predict = []\n",
    "\n",
    "for m in stacking_model:\n",
    "    if m == 'KNN':\n",
    "        continue\n",
    "    print('Start predict %s model...'%m)\n",
    "    for i in range(len(label_list)):\n",
    "        model = joblib.load('%s%s_%s.model'%(temp_file_path+'model/', m, label_list[i]))\n",
    "        train_set_predict.append(model.predict(X_train))\n",
    "        test_set_predict.append(model.predict(X_test))\n",
    "if 'KNN' in stacking_model:\n",
    "    print('Start predict %s model...'%'KNN')\n",
    "    for i in range(len(label_list)):\n",
    "        y_pred1 = [KNN_weighted_regression(X_train, y_train[i] , x, k=len(X_train), metric='cosine') for x in X_train]\n",
    "        y_pred2 = [KNN_weighted_regression(X_train, y_train[i] , x, k=len(X_train), metric='cosine') for x in X_test]\n",
    "        train_set_predict.append(y_pred1)\n",
    "        test_set_predict.append(y_pred2)            \n",
    "            \n",
    "        \n",
    "train_set_predict = np.asarray(train_set_predict)\n",
    "test_set_predict = np.asarray(test_set_predict)\n",
    "\n",
    "result = {}\n",
    "rmse = []\n",
    "model_type = 'LGBM'\n",
    "validation = False\n",
    "\n",
    "X_train_s = train_set_predict.T\n",
    "y_train_s = y_train.T\n",
    "    \n",
    "print(X_train_s.shape)\n",
    "epochs = [70, 100, 70, 70, 50]\n",
    "for i in range(len(label_list)):\n",
    "    if model_type == 'MLP': \n",
    "    # —————————————————--------—— MLP ——————————————--—————————————\n",
    "        num_iter = [500, 500, 500, 500, 100]\n",
    "        model = MLPRegressor(hidden_layer_sizes=(20), max_iter=num_iter[i], activation='relu', solver='lbfgs',\n",
    "                             random_state=0)\n",
    "        model.fit(X_train_s, y_train_s.T[i])\n",
    "    # —————————————————————————————————————————————————————-\n",
    "    elif model_type == 'SVR':\n",
    "    # —————————————————--------—— SVR ——————————————--—————————————\n",
    "        model = SVR(gamma='scale')\n",
    "        model.fit(X_train_s, y_train_s.T[i])\n",
    "    # ————————————————————————————————————-—————————————————\n",
    "    elif model_type == 'LR':\n",
    "    # —————————————————--------——- LR ——————————————--—————————————\n",
    "        model = LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)\n",
    "        model.fit(X_train_s, y_train_s.T[i])\n",
    "    # —————————————————--------————-——————————————--—————————————\n",
    "    elif model_type == 'LGBM':\n",
    "    # —————————————————--------—— LGBM ————————————————————————————\n",
    "        model = lgb.LGBMRegressor(objective='regression',max_depth=-1, num_leaves=64,learning_rate=0.05, n_estimators=epochs[i])\n",
    "        if validation:\n",
    "            model.fit(X_train_s, y_train_s.T[i], eval_set=[(X_valid_s, y_valid_s.T[i])],eval_metric='rmse',\\\n",
    "                      early_stopping_rounds=10, verbose=False)\n",
    "        else:\n",
    "            model.fit(X_train_s, y_train_s.T[i], verbose=False)\n",
    "    # —————————————————--------————-——————————————--—————————————\n",
    "    result[label_list[i]] = model.predict(test_set_predict.T)\n",
    "\n",
    "result_df = pd.DataFrame.copy(y_test_df)    \n",
    "for col in result.keys():\n",
    "    result_df[col] = result[col]\n",
    "\n",
    "result_df = result_df.drop(['product_batch'],axis=1)\n",
    "result_df.to_csv(temp_file_path+'stacking_result.csv', index=False, encoding='utf-8')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result merge\n",
    "df1 = pd.read_csv('0.288429.csv', header=0, encoding='gbk')\n",
    "df2 = pd.read_csv('0.28992.csv', header=0, encoding='gbk')\n",
    "\n",
    "df1['total_nutrient'] = df1['phosphorus_content'] + df1['nitrogen_content']\n",
    "df2['total_nutrient'] = df2['phosphorus_content'] + df2['nitrogen_content']\n",
    "\n",
    "weights = [0.288429, 0.28992]\n",
    "inverse_sum = sum([1/w for w in weights])\n",
    "for c in df1.columns.values[1:]:\n",
    "    df1[c] = (df1[c] / weights[0] + df2[c] /weights[1])/inverse_sum\n",
    "\n",
    "df1.to_csv('merge_result.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
